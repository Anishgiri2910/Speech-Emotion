{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imports \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import random\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "##Keras\n",
    "import pydot\n",
    "import graphviz\n",
    "#import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dropout,Average\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "#from keras.layers.conlutional import Conv1D\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n",
    "#from keras.models import Model, Sequential\n",
    "#from keras.layers import Dense, Embedding, LSTM, Add,Average\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "#from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.layers import Input, Conv1D, Conv2D,BatchNormalization, MaxPooling1D,MaxPooling2D, LSTM, Dense, Activation, Layer,Reshape,Concatenate\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import SVC\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "             discriminant_analysis, random_projection)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.patheffects as pe\n",
    "from sklearn.manifold import TSNE \n",
    "import seaborn as sb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading files\n",
    "files=os.listdir('E:/Audio_full/')\n",
    "## Data sets\n",
    "# - RAVDESS Both speech and song with 8 emotions 2364 files in total with 24 actors each\n",
    "# - TESS 7 emotions 2800 files in total\n",
    "# - SAVEE 4 Actors 480 files in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in d:\\anaconda3\\lib\\site-packages (0.16)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feeling dataset of corresponding audio\n",
    "\n",
    "feeling_list=[]\n",
    "for item in files:\n",
    "    if item[6:-16]=='01' or item[2]=='n' or item[-11:-4]=='neutral' or item[6:-16]=='02':\n",
    "        feeling_list.append('neutral')\n",
    "    #elif :\n",
    "        #feeling_list.append('calm')\n",
    "    elif item[6:-16]=='03' or item[2]=='h' or item[-9:-4]=='happy':\n",
    "        feeling_list.append('happy')\n",
    "    elif item[6:-16]=='04' or item[2:4]=='sa' or item[-7:-4]=='sad':\n",
    "        feeling_list.append('sad')\n",
    "    elif item[6:-16]=='05' or item[2]=='a' or item[-9:-4]=='angry':\n",
    "        feeling_list.append('angry')\n",
    "    elif item[6:-16]=='07' or item[2]=='d' or item[-11:-4]=='disgust':\n",
    "        feeling_list.append('disgust')\n",
    "    elif item[6:-16]=='06' or item[2]=='f'or item[-8:-4]=='fear':\n",
    "        feeling_list.append('fear')\n",
    "    elif item[6:-16]=='08' or item[2:4]=='su' or item[-6:-4]=='ps':\n",
    "        feeling_list.append('surprised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5642</th>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotions\n",
       "0       neutral\n",
       "1       neutral\n",
       "2       neutral\n",
       "3       neutral\n",
       "4       neutral\n",
       "...         ...\n",
       "5639       fear\n",
       "5640      happy\n",
       "5641    neutral\n",
       "5642  surprised\n",
       "5643        sad\n",
       "\n",
       "[5644 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame(feeling_list,columns=['emotions'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-700.3989, 58.63021, -3.025852, 16.040241, 4....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-653.169, 58.028076, -12.581207, 11.818786, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-617.0523, 60.10337, -5.9842577, 13.886285, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-698.7211, 47.088486, -11.333614, 12.963088, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-715.9226, 71.9768, 4.457525, 18.602499, 8.40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>[-343.40387, 32.59755, -9.197987, 25.315773, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>[-345.47714, 24.427338, -14.398651, 36.95299, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>[-416.72632, 71.14924, -1.1160216, 42.52479, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5642</th>\n",
       "      <td>[-374.82407, 46.681873, -12.381617, 18.139084,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>[-414.83298, 61.24435, 24.871048, 47.345543, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature\n",
       "0     [-700.3989, 58.63021, -3.025852, 16.040241, 4....\n",
       "1     [-653.169, 58.028076, -12.581207, 11.818786, -...\n",
       "2     [-617.0523, 60.10337, -5.9842577, 13.886285, 1...\n",
       "3     [-698.7211, 47.088486, -11.333614, 12.963088, ...\n",
       "4     [-715.9226, 71.9768, 4.457525, 18.602499, 8.40...\n",
       "...                                                 ...\n",
       "5639  [-343.40387, 32.59755, -9.197987, 25.315773, 0...\n",
       "5640  [-345.47714, 24.427338, -14.398651, 36.95299, ...\n",
       "5641  [-416.72632, 71.14924, -1.1160216, 42.52479, -...\n",
       "5642  [-374.82407, 46.681873, -12.381617, 18.139084,...\n",
       "5643  [-414.83298, 61.24435, 24.871048, 47.345543, -...\n",
       "\n",
       "[5644 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Converting the audio to data using MFCC PART 1\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for index,y in enumerate(files):\n",
    "    X, sample_rate = librosa.load('E:/Audio_full/'+y, res_type='kaiser_fast')\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X,sr=sample_rate, n_mfcc=50).T,axis=0)\n",
    "    feature = mfccs\n",
    "    df.loc[bookmark] = [feature]\n",
    "    bookmark=bookmark+1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-700.398926</td>\n",
       "      <td>58.630211</td>\n",
       "      <td>-3.025852</td>\n",
       "      <td>16.040241</td>\n",
       "      <td>4.248529</td>\n",
       "      <td>3.869935</td>\n",
       "      <td>-6.381716</td>\n",
       "      <td>-0.188635</td>\n",
       "      <td>-13.735005</td>\n",
       "      <td>-0.319724</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.432791</td>\n",
       "      <td>-1.268412</td>\n",
       "      <td>-2.037442</td>\n",
       "      <td>-3.208719</td>\n",
       "      <td>-2.298195</td>\n",
       "      <td>-2.391170</td>\n",
       "      <td>-2.533661</td>\n",
       "      <td>-0.790085</td>\n",
       "      <td>-2.240257</td>\n",
       "      <td>-1.202297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-653.169006</td>\n",
       "      <td>58.028076</td>\n",
       "      <td>-12.581207</td>\n",
       "      <td>11.818786</td>\n",
       "      <td>-7.681562</td>\n",
       "      <td>-0.617142</td>\n",
       "      <td>-8.337758</td>\n",
       "      <td>-5.823570</td>\n",
       "      <td>-6.547592</td>\n",
       "      <td>1.458057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.561838</td>\n",
       "      <td>-0.344248</td>\n",
       "      <td>-0.597152</td>\n",
       "      <td>-2.483091</td>\n",
       "      <td>-2.482294</td>\n",
       "      <td>-1.513263</td>\n",
       "      <td>-0.206973</td>\n",
       "      <td>-2.042859</td>\n",
       "      <td>-2.453341</td>\n",
       "      <td>-2.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-617.052307</td>\n",
       "      <td>60.103371</td>\n",
       "      <td>-5.984258</td>\n",
       "      <td>13.886285</td>\n",
       "      <td>1.120427</td>\n",
       "      <td>0.511750</td>\n",
       "      <td>-14.841358</td>\n",
       "      <td>-4.016369</td>\n",
       "      <td>-5.575839</td>\n",
       "      <td>-6.309851</td>\n",
       "      <td>...</td>\n",
       "      <td>3.856137</td>\n",
       "      <td>2.830290</td>\n",
       "      <td>0.607302</td>\n",
       "      <td>-0.470186</td>\n",
       "      <td>0.242361</td>\n",
       "      <td>-0.506050</td>\n",
       "      <td>0.155130</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>0.215989</td>\n",
       "      <td>0.503251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-698.721130</td>\n",
       "      <td>47.088486</td>\n",
       "      <td>-11.333614</td>\n",
       "      <td>12.963088</td>\n",
       "      <td>-8.005651</td>\n",
       "      <td>-1.252229</td>\n",
       "      <td>-10.009068</td>\n",
       "      <td>-9.434125</td>\n",
       "      <td>-10.318875</td>\n",
       "      <td>-0.864674</td>\n",
       "      <td>...</td>\n",
       "      <td>3.345852</td>\n",
       "      <td>-0.199447</td>\n",
       "      <td>0.761450</td>\n",
       "      <td>0.445034</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>-0.074606</td>\n",
       "      <td>-0.224912</td>\n",
       "      <td>-1.751279</td>\n",
       "      <td>-0.997531</td>\n",
       "      <td>0.558003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-715.922607</td>\n",
       "      <td>71.976799</td>\n",
       "      <td>4.457525</td>\n",
       "      <td>18.602499</td>\n",
       "      <td>8.404046</td>\n",
       "      <td>3.621954</td>\n",
       "      <td>-1.974365</td>\n",
       "      <td>0.022672</td>\n",
       "      <td>-2.678083</td>\n",
       "      <td>4.232803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427559</td>\n",
       "      <td>0.132589</td>\n",
       "      <td>-1.459148</td>\n",
       "      <td>-2.129129</td>\n",
       "      <td>-1.101375</td>\n",
       "      <td>1.795965</td>\n",
       "      <td>2.756026</td>\n",
       "      <td>1.120542</td>\n",
       "      <td>1.270001</td>\n",
       "      <td>1.677747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>-343.403870</td>\n",
       "      <td>32.597549</td>\n",
       "      <td>-9.197987</td>\n",
       "      <td>25.315773</td>\n",
       "      <td>0.052862</td>\n",
       "      <td>0.853997</td>\n",
       "      <td>-11.733653</td>\n",
       "      <td>0.140329</td>\n",
       "      <td>-25.528193</td>\n",
       "      <td>4.334096</td>\n",
       "      <td>...</td>\n",
       "      <td>6.829886</td>\n",
       "      <td>1.623786</td>\n",
       "      <td>0.174217</td>\n",
       "      <td>-2.333187</td>\n",
       "      <td>0.229022</td>\n",
       "      <td>-2.646233</td>\n",
       "      <td>-0.123384</td>\n",
       "      <td>2.442371</td>\n",
       "      <td>1.630154</td>\n",
       "      <td>-2.707299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>-345.477142</td>\n",
       "      <td>24.427338</td>\n",
       "      <td>-14.398651</td>\n",
       "      <td>36.952991</td>\n",
       "      <td>-7.297604</td>\n",
       "      <td>1.008584</td>\n",
       "      <td>-15.700944</td>\n",
       "      <td>-10.525764</td>\n",
       "      <td>-6.183089</td>\n",
       "      <td>-6.078100</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.621850</td>\n",
       "      <td>-5.776743</td>\n",
       "      <td>-3.046839</td>\n",
       "      <td>0.392119</td>\n",
       "      <td>0.650877</td>\n",
       "      <td>1.291245</td>\n",
       "      <td>0.535487</td>\n",
       "      <td>2.879143</td>\n",
       "      <td>0.849994</td>\n",
       "      <td>-6.130474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>-416.726318</td>\n",
       "      <td>71.149239</td>\n",
       "      <td>-1.116022</td>\n",
       "      <td>42.524792</td>\n",
       "      <td>-6.296183</td>\n",
       "      <td>2.258823</td>\n",
       "      <td>-6.628646</td>\n",
       "      <td>-7.701837</td>\n",
       "      <td>-10.190816</td>\n",
       "      <td>7.239936</td>\n",
       "      <td>...</td>\n",
       "      <td>7.608363</td>\n",
       "      <td>6.693035</td>\n",
       "      <td>-1.709971</td>\n",
       "      <td>-7.858405</td>\n",
       "      <td>-4.653687</td>\n",
       "      <td>2.703166</td>\n",
       "      <td>4.437060</td>\n",
       "      <td>-1.495853</td>\n",
       "      <td>-8.279791</td>\n",
       "      <td>-6.083230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5642</th>\n",
       "      <td>-374.824066</td>\n",
       "      <td>46.681873</td>\n",
       "      <td>-12.381617</td>\n",
       "      <td>18.139084</td>\n",
       "      <td>-22.662424</td>\n",
       "      <td>4.170814</td>\n",
       "      <td>-15.546303</td>\n",
       "      <td>-7.580999</td>\n",
       "      <td>-18.976576</td>\n",
       "      <td>6.013675</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192490</td>\n",
       "      <td>1.591670</td>\n",
       "      <td>1.171150</td>\n",
       "      <td>0.643918</td>\n",
       "      <td>0.412878</td>\n",
       "      <td>-1.108500</td>\n",
       "      <td>1.322803</td>\n",
       "      <td>-0.409489</td>\n",
       "      <td>1.067348</td>\n",
       "      <td>-0.033086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>-414.832977</td>\n",
       "      <td>61.244350</td>\n",
       "      <td>24.871048</td>\n",
       "      <td>47.345543</td>\n",
       "      <td>-4.288396</td>\n",
       "      <td>13.609493</td>\n",
       "      <td>-9.357813</td>\n",
       "      <td>-4.297829</td>\n",
       "      <td>-3.560240</td>\n",
       "      <td>-3.150320</td>\n",
       "      <td>...</td>\n",
       "      <td>2.989090</td>\n",
       "      <td>0.215732</td>\n",
       "      <td>0.194141</td>\n",
       "      <td>-4.121280</td>\n",
       "      <td>-1.334297</td>\n",
       "      <td>-1.362383</td>\n",
       "      <td>-0.006421</td>\n",
       "      <td>-1.341372</td>\n",
       "      <td>-1.434265</td>\n",
       "      <td>1.495095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5   \\\n",
       "0    -700.398926  58.630211  -3.025852  16.040241   4.248529   3.869935   \n",
       "1    -653.169006  58.028076 -12.581207  11.818786  -7.681562  -0.617142   \n",
       "2    -617.052307  60.103371  -5.984258  13.886285   1.120427   0.511750   \n",
       "3    -698.721130  47.088486 -11.333614  12.963088  -8.005651  -1.252229   \n",
       "4    -715.922607  71.976799   4.457525  18.602499   8.404046   3.621954   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "5639 -343.403870  32.597549  -9.197987  25.315773   0.052862   0.853997   \n",
       "5640 -345.477142  24.427338 -14.398651  36.952991  -7.297604   1.008584   \n",
       "5641 -416.726318  71.149239  -1.116022  42.524792  -6.296183   2.258823   \n",
       "5642 -374.824066  46.681873 -12.381617  18.139084 -22.662424   4.170814   \n",
       "5643 -414.832977  61.244350  24.871048  47.345543  -4.288396  13.609493   \n",
       "\n",
       "             6          7          8         9   ...        40        41  \\\n",
       "0     -6.381716  -0.188635 -13.735005 -0.319724  ... -3.432791 -1.268412   \n",
       "1     -8.337758  -5.823570  -6.547592  1.458057  ... -0.561838 -0.344248   \n",
       "2    -14.841358  -4.016369  -5.575839 -6.309851  ...  3.856137  2.830290   \n",
       "3    -10.009068  -9.434125 -10.318875 -0.864674  ...  3.345852 -0.199447   \n",
       "4     -1.974365   0.022672  -2.678083  4.232803  ...  0.427559  0.132589   \n",
       "...         ...        ...        ...       ...  ...       ...       ...   \n",
       "5639 -11.733653   0.140329 -25.528193  4.334096  ...  6.829886  1.623786   \n",
       "5640 -15.700944 -10.525764  -6.183089 -6.078100  ... -1.621850 -5.776743   \n",
       "5641  -6.628646  -7.701837 -10.190816  7.239936  ...  7.608363  6.693035   \n",
       "5642 -15.546303  -7.580999 -18.976576  6.013675  ...  1.192490  1.591670   \n",
       "5643  -9.357813  -4.297829  -3.560240 -3.150320  ...  2.989090  0.215732   \n",
       "\n",
       "            42        43        44        45        46        47        48  \\\n",
       "0    -2.037442 -3.208719 -2.298195 -2.391170 -2.533661 -0.790085 -2.240257   \n",
       "1    -0.597152 -2.483091 -2.482294 -1.513263 -0.206973 -2.042859 -2.453341   \n",
       "2     0.607302 -0.470186  0.242361 -0.506050  0.155130  1.041393  0.215989   \n",
       "3     0.761450  0.445034  0.017971 -0.074606 -0.224912 -1.751279 -0.997531   \n",
       "4    -1.459148 -2.129129 -1.101375  1.795965  2.756026  1.120542  1.270001   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5639  0.174217 -2.333187  0.229022 -2.646233 -0.123384  2.442371  1.630154   \n",
       "5640 -3.046839  0.392119  0.650877  1.291245  0.535487  2.879143  0.849994   \n",
       "5641 -1.709971 -7.858405 -4.653687  2.703166  4.437060 -1.495853 -8.279791   \n",
       "5642  1.171150  0.643918  0.412878 -1.108500  1.322803 -0.409489  1.067348   \n",
       "5643  0.194141 -4.121280 -1.334297 -1.362383 -0.006421 -1.341372 -1.434265   \n",
       "\n",
       "            49  \n",
       "0    -1.202297  \n",
       "1    -2.000150  \n",
       "2     0.503251  \n",
       "3     0.558003  \n",
       "4     1.677747  \n",
       "...        ...  \n",
       "5639 -2.707299  \n",
       "5640 -6.130474  \n",
       "5641 -6.083230  \n",
       "5642 -0.033086  \n",
       "5643  1.495095  \n",
       "\n",
       "[5644 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.DataFrame(df['feature'].values.tolist())\n",
    "df2=df2.fillna(0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.3591023e-06, 5.603021e-05, 0.003625807, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.2332691e-05, 5.2230745e-05, 0.00011250672, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2.6729702e-05, 0.00019209618, 0.0007873292, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[6.656757e-06, 9.897744e-06, 1.4620192e-05, 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.00011870869, 0.00011706894, 0.0011368697, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>[0.088534385, 0.032074165, 0.01548406, 0.01437...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>[0.0037470474, 0.0011213432, 0.0011808255, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>[0.009096331, 0.0084486315, 0.008483385, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5642</th>\n",
       "      <td>[0.019076487, 0.001114162, 0.0003238535, 0.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>[0.08253205, 0.136046, 0.0933211, 0.062120136,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               feature1\n",
       "0     [1.3591023e-06, 5.603021e-05, 0.003625807, 0.0...\n",
       "1     [1.2332691e-05, 5.2230745e-05, 0.00011250672, ...\n",
       "2     [2.6729702e-05, 0.00019209618, 0.0007873292, 0...\n",
       "3     [6.656757e-06, 9.897744e-06, 1.4620192e-05, 7....\n",
       "4     [0.00011870869, 0.00011706894, 0.0011368697, 0...\n",
       "...                                                 ...\n",
       "5639  [0.088534385, 0.032074165, 0.01548406, 0.01437...\n",
       "5640  [0.0037470474, 0.0011213432, 0.0011808255, 0.0...\n",
       "5641  [0.009096331, 0.0084486315, 0.008483385, 0.009...\n",
       "5642  [0.019076487, 0.001114162, 0.0003238535, 0.000...\n",
       "5643  [0.08253205, 0.136046, 0.0933211, 0.062120136,...\n",
       "\n",
       "[5644 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MEL spectogram part2\n",
    "dfmelSp = pd.DataFrame(columns=['feature1'])\n",
    "bookmark=0\n",
    "for index,y in enumerate(files):\n",
    "    X, sample_rate = librosa.load('E:/Audio_full/'+y,res_type='kaiser_fast')\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mel= np.mean(librosa.feature.melspectrogram(y=X,sr=sample_rate).T,axis=0)\n",
    "    feature = mel\n",
    "    dfmelSp.loc[bookmark] = [feature]\n",
    "    bookmark=bookmark+1\n",
    "dfmelSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.020956</td>\n",
       "      <td>0.031414</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.018618</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>0.020465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>6.755617e-07</td>\n",
       "      <td>3.586787e-07</td>\n",
       "      <td>3.909842e-07</td>\n",
       "      <td>2.609652e-07</td>\n",
       "      <td>6.226127e-08</td>\n",
       "      <td>1.362780e-08</td>\n",
       "      <td>1.109996e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.010611</td>\n",
       "      <td>0.067382</td>\n",
       "      <td>0.132326</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.351835e-06</td>\n",
       "      <td>2.065444e-06</td>\n",
       "      <td>5.446865e-07</td>\n",
       "      <td>2.222711e-07</td>\n",
       "      <td>7.489701e-08</td>\n",
       "      <td>1.204194e-08</td>\n",
       "      <td>1.612842e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>0.046933</td>\n",
       "      <td>0.067469</td>\n",
       "      <td>0.030527</td>\n",
       "      <td>0.016795</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1.127173e-05</td>\n",
       "      <td>6.057411e-06</td>\n",
       "      <td>2.383926e-06</td>\n",
       "      <td>1.396174e-06</td>\n",
       "      <td>7.454358e-07</td>\n",
       "      <td>1.715608e-07</td>\n",
       "      <td>2.231222e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>0.032546</td>\n",
       "      <td>0.021561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>7.791955e-06</td>\n",
       "      <td>5.814118e-06</td>\n",
       "      <td>1.717594e-06</td>\n",
       "      <td>7.783661e-07</td>\n",
       "      <td>1.526976e-07</td>\n",
       "      <td>2.095097e-08</td>\n",
       "      <td>2.928901e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>0.014026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>9.353384e-07</td>\n",
       "      <td>4.004243e-07</td>\n",
       "      <td>2.418668e-07</td>\n",
       "      <td>7.589256e-08</td>\n",
       "      <td>2.278530e-08</td>\n",
       "      <td>4.165693e-09</td>\n",
       "      <td>3.284917e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>0.088534</td>\n",
       "      <td>0.032074</td>\n",
       "      <td>0.015484</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0.521248</td>\n",
       "      <td>2.002649</td>\n",
       "      <td>0.849206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>3.691630e-03</td>\n",
       "      <td>1.624443e-03</td>\n",
       "      <td>3.285170e-04</td>\n",
       "      <td>2.007679e-04</td>\n",
       "      <td>6.891089e-05</td>\n",
       "      <td>2.034424e-05</td>\n",
       "      <td>3.410880e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.266760</td>\n",
       "      <td>2.143342</td>\n",
       "      <td>3.968316</td>\n",
       "      <td>3.609105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>2.874391e-03</td>\n",
       "      <td>2.240580e-03</td>\n",
       "      <td>6.783053e-04</td>\n",
       "      <td>1.343765e-04</td>\n",
       "      <td>7.181308e-05</td>\n",
       "      <td>3.513732e-05</td>\n",
       "      <td>1.744838e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>0.009096</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>0.008483</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>2.562184</td>\n",
       "      <td>7.563576</td>\n",
       "      <td>0.340539</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>2.352791e-03</td>\n",
       "      <td>9.355015e-04</td>\n",
       "      <td>1.696493e-04</td>\n",
       "      <td>5.026917e-05</td>\n",
       "      <td>1.020348e-05</td>\n",
       "      <td>4.442834e-06</td>\n",
       "      <td>4.414226e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5642</th>\n",
       "      <td>0.019076</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.104751</td>\n",
       "      <td>0.308540</td>\n",
       "      <td>0.358288</td>\n",
       "      <td>0.242882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>1.932580e-03</td>\n",
       "      <td>1.206304e-03</td>\n",
       "      <td>1.850308e-04</td>\n",
       "      <td>8.864246e-05</td>\n",
       "      <td>1.673799e-05</td>\n",
       "      <td>6.823322e-06</td>\n",
       "      <td>6.962310e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>0.082532</td>\n",
       "      <td>0.136046</td>\n",
       "      <td>0.093321</td>\n",
       "      <td>0.062120</td>\n",
       "      <td>0.031951</td>\n",
       "      <td>1.264406</td>\n",
       "      <td>6.724216</td>\n",
       "      <td>6.134419</td>\n",
       "      <td>0.560074</td>\n",
       "      <td>4.111078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>0.004833</td>\n",
       "      <td>6.911867e-03</td>\n",
       "      <td>4.034434e-03</td>\n",
       "      <td>7.906675e-04</td>\n",
       "      <td>1.472316e-04</td>\n",
       "      <td>6.147699e-05</td>\n",
       "      <td>1.457585e-05</td>\n",
       "      <td>1.711012e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.000001  0.000056  0.003626  0.020956  0.031414  0.017692  0.003454   \n",
       "1     0.000012  0.000052  0.000113  0.000077  0.000056  0.000892  0.010611   \n",
       "2     0.000027  0.000192  0.000787  0.002619  0.005716  0.046933  0.067469   \n",
       "3     0.000007  0.000010  0.000015  0.000008  0.000015  0.003372  0.008763   \n",
       "4     0.000119  0.000117  0.001137  0.003581  0.013649  0.008650  0.001231   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5639  0.088534  0.032074  0.015484  0.014378  0.005388  0.002551  0.009657   \n",
       "5640  0.003747  0.001121  0.001181  0.001105  0.000706  0.001211  0.266760   \n",
       "5641  0.009096  0.008449  0.008483  0.009777  0.008108  0.011599  2.562184   \n",
       "5642  0.019076  0.001114  0.000324  0.000266  0.000249  0.001760  0.104751   \n",
       "5643  0.082532  0.136046  0.093321  0.062120  0.031951  1.264406  6.724216   \n",
       "\n",
       "           7         8         9    ...       118       119       120  \\\n",
       "0     0.018618  0.024684  0.020465  ...  0.000002  0.000002  0.000001   \n",
       "1     0.067382  0.132326  0.016235  ...  0.000013  0.000009  0.000007   \n",
       "2     0.030527  0.016795  0.009505  ...  0.000045  0.000035  0.000022   \n",
       "3     0.014438  0.032546  0.021561  ...  0.000031  0.000026  0.000020   \n",
       "4     0.002388  0.004787  0.014026  ...  0.000003  0.000002  0.000002   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5639  0.521248  2.002649  0.849206  ...  0.004074  0.003433  0.003914   \n",
       "5640  2.143342  3.968316  3.609105  ...  0.005176  0.005318  0.003357   \n",
       "5641  7.563576  0.340539  0.011818  ...  0.001646  0.002988  0.003047   \n",
       "5642  0.308540  0.358288  0.242882  ...  0.001825  0.002067  0.002671   \n",
       "5643  6.134419  0.560074  4.111078  ...  0.004569  0.005343  0.004833   \n",
       "\n",
       "               121           122           123           124           125  \\\n",
       "0     6.755617e-07  3.586787e-07  3.909842e-07  2.609652e-07  6.226127e-08   \n",
       "1     3.351835e-06  2.065444e-06  5.446865e-07  2.222711e-07  7.489701e-08   \n",
       "2     1.127173e-05  6.057411e-06  2.383926e-06  1.396174e-06  7.454358e-07   \n",
       "3     7.791955e-06  5.814118e-06  1.717594e-06  7.783661e-07  1.526976e-07   \n",
       "4     9.353384e-07  4.004243e-07  2.418668e-07  7.589256e-08  2.278530e-08   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "5639  3.691630e-03  1.624443e-03  3.285170e-04  2.007679e-04  6.891089e-05   \n",
       "5640  2.874391e-03  2.240580e-03  6.783053e-04  1.343765e-04  7.181308e-05   \n",
       "5641  2.352791e-03  9.355015e-04  1.696493e-04  5.026917e-05  1.020348e-05   \n",
       "5642  1.932580e-03  1.206304e-03  1.850308e-04  8.864246e-05  1.673799e-05   \n",
       "5643  6.911867e-03  4.034434e-03  7.906675e-04  1.472316e-04  6.147699e-05   \n",
       "\n",
       "               126           127  \n",
       "0     1.362780e-08  1.109996e-09  \n",
       "1     1.204194e-08  1.612842e-09  \n",
       "2     1.715608e-07  2.231222e-08  \n",
       "3     2.095097e-08  2.928901e-09  \n",
       "4     4.165693e-09  3.284917e-10  \n",
       "...            ...           ...  \n",
       "5639  2.034424e-05  3.410880e-06  \n",
       "5640  3.513732e-05  1.744838e-05  \n",
       "5641  4.442834e-06  4.414226e-07  \n",
       "5642  6.823322e-06  6.962310e-07  \n",
       "5643  1.457585e-05  1.711012e-06  \n",
       "\n",
       "[5644 rows x 128 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmelSp2=pd.DataFrame(dfmelSp['feature1'].values.tolist())\n",
    "dfmelSp2=dfmelSp2.fillna(0)\n",
    "dfmelSp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         neutral\n",
       "1         neutral\n",
       "2         neutral\n",
       "3         neutral\n",
       "4         neutral\n",
       "          ...    \n",
       "5639         fear\n",
       "5640        happy\n",
       "5641      neutral\n",
       "5642    surprised\n",
       "5643          sad\n",
       "Name: emotions, Length: 5644, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>49</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.202297</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.000150</td>\n",
       "      <td>0.005754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503251</td>\n",
       "      <td>0.003092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.558003</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.677747</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>-2.707299</td>\n",
       "      <td>2.584399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>-6.130474</td>\n",
       "      <td>0.004410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>-6.083230</td>\n",
       "      <td>0.001173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5642</th>\n",
       "      <td>-0.033086</td>\n",
       "      <td>0.035044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>1.495095</td>\n",
       "      <td>0.000548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            49        49\n",
       "0    -1.202297  0.000644\n",
       "1    -2.000150  0.005754\n",
       "2     0.503251  0.003092\n",
       "3     0.558003  0.000424\n",
       "4     1.677747  0.000418\n",
       "...        ...       ...\n",
       "5639 -2.707299  2.584399\n",
       "5640 -6.130474  0.004410\n",
       "5641 -6.083230  0.001173\n",
       "5642 -0.033086  0.035044\n",
       "5643  1.495095  0.000548\n",
       "\n",
       "[5644 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf=pd.concat([df2,labels,dfmelSp2],axis=1)\n",
    "newdf['emotions']\n",
    "newdf[49]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5644, 179)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>-395.827271</td>\n",
       "      <td>109.815002</td>\n",
       "      <td>26.334900</td>\n",
       "      <td>34.032913</td>\n",
       "      <td>15.383346</td>\n",
       "      <td>3.612015</td>\n",
       "      <td>-21.559561</td>\n",
       "      <td>-1.922961</td>\n",
       "      <td>-1.025764</td>\n",
       "      <td>-10.060621</td>\n",
       "      <td>...</td>\n",
       "      <td>5.988093e-06</td>\n",
       "      <td>1.842506e-06</td>\n",
       "      <td>5.611226e-07</td>\n",
       "      <td>4.509578e-07</td>\n",
       "      <td>4.783789e-07</td>\n",
       "      <td>5.055595e-07</td>\n",
       "      <td>5.169114e-07</td>\n",
       "      <td>5.141086e-07</td>\n",
       "      <td>5.031247e-07</td>\n",
       "      <td>4.917418e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>-477.751160</td>\n",
       "      <td>95.906578</td>\n",
       "      <td>9.930469</td>\n",
       "      <td>8.222509</td>\n",
       "      <td>-11.132051</td>\n",
       "      <td>2.174797</td>\n",
       "      <td>-22.116783</td>\n",
       "      <td>3.972046</td>\n",
       "      <td>-24.781950</td>\n",
       "      <td>1.014994</td>\n",
       "      <td>...</td>\n",
       "      <td>1.594053e-04</td>\n",
       "      <td>1.025494e-04</td>\n",
       "      <td>7.045877e-05</td>\n",
       "      <td>5.231352e-05</td>\n",
       "      <td>2.467163e-05</td>\n",
       "      <td>1.023444e-05</td>\n",
       "      <td>1.734145e-06</td>\n",
       "      <td>4.149469e-07</td>\n",
       "      <td>8.510043e-08</td>\n",
       "      <td>1.513139e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>-635.511292</td>\n",
       "      <td>143.350479</td>\n",
       "      <td>31.454308</td>\n",
       "      <td>36.235615</td>\n",
       "      <td>11.199054</td>\n",
       "      <td>-6.821490</td>\n",
       "      <td>-4.445740</td>\n",
       "      <td>2.376345</td>\n",
       "      <td>-8.011057</td>\n",
       "      <td>-12.235952</td>\n",
       "      <td>...</td>\n",
       "      <td>1.572271e-08</td>\n",
       "      <td>9.987369e-09</td>\n",
       "      <td>6.427498e-09</td>\n",
       "      <td>4.529917e-09</td>\n",
       "      <td>3.163819e-09</td>\n",
       "      <td>2.177597e-09</td>\n",
       "      <td>1.774798e-09</td>\n",
       "      <td>1.502753e-09</td>\n",
       "      <td>1.398907e-09</td>\n",
       "      <td>1.364231e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>-338.146210</td>\n",
       "      <td>109.372353</td>\n",
       "      <td>13.920223</td>\n",
       "      <td>33.369545</td>\n",
       "      <td>5.132312</td>\n",
       "      <td>2.439946</td>\n",
       "      <td>-28.102674</td>\n",
       "      <td>-6.727801</td>\n",
       "      <td>-0.976052</td>\n",
       "      <td>-15.288651</td>\n",
       "      <td>...</td>\n",
       "      <td>6.443388e-06</td>\n",
       "      <td>1.800071e-06</td>\n",
       "      <td>9.294707e-07</td>\n",
       "      <td>8.245880e-07</td>\n",
       "      <td>8.740222e-07</td>\n",
       "      <td>9.181483e-07</td>\n",
       "      <td>9.375869e-07</td>\n",
       "      <td>9.326237e-07</td>\n",
       "      <td>9.153359e-07</td>\n",
       "      <td>8.970123e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>-444.435394</td>\n",
       "      <td>70.364319</td>\n",
       "      <td>14.762432</td>\n",
       "      <td>19.777376</td>\n",
       "      <td>6.337432</td>\n",
       "      <td>10.598396</td>\n",
       "      <td>-18.977951</td>\n",
       "      <td>5.523309</td>\n",
       "      <td>-22.413912</td>\n",
       "      <td>1.024355</td>\n",
       "      <td>...</td>\n",
       "      <td>7.279449e-04</td>\n",
       "      <td>3.718181e-04</td>\n",
       "      <td>2.150821e-04</td>\n",
       "      <td>1.537552e-04</td>\n",
       "      <td>8.546136e-05</td>\n",
       "      <td>3.408233e-05</td>\n",
       "      <td>9.490821e-06</td>\n",
       "      <td>1.508427e-06</td>\n",
       "      <td>2.611371e-07</td>\n",
       "      <td>2.310411e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>-616.977356</td>\n",
       "      <td>65.106079</td>\n",
       "      <td>-44.349960</td>\n",
       "      <td>19.407906</td>\n",
       "      <td>-2.108910</td>\n",
       "      <td>-20.398224</td>\n",
       "      <td>-19.426634</td>\n",
       "      <td>-10.607098</td>\n",
       "      <td>-6.794960</td>\n",
       "      <td>2.840420</td>\n",
       "      <td>...</td>\n",
       "      <td>8.187031e-06</td>\n",
       "      <td>5.235459e-06</td>\n",
       "      <td>2.537881e-06</td>\n",
       "      <td>9.923816e-07</td>\n",
       "      <td>1.467780e-06</td>\n",
       "      <td>1.440692e-06</td>\n",
       "      <td>4.680294e-07</td>\n",
       "      <td>1.425603e-07</td>\n",
       "      <td>2.132506e-08</td>\n",
       "      <td>2.032422e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>-485.176361</td>\n",
       "      <td>20.533010</td>\n",
       "      <td>-23.324465</td>\n",
       "      <td>-0.686083</td>\n",
       "      <td>-21.678753</td>\n",
       "      <td>-7.451090</td>\n",
       "      <td>-15.483243</td>\n",
       "      <td>-9.952831</td>\n",
       "      <td>-26.001814</td>\n",
       "      <td>2.710965</td>\n",
       "      <td>...</td>\n",
       "      <td>5.375385e-04</td>\n",
       "      <td>4.639467e-04</td>\n",
       "      <td>2.838194e-04</td>\n",
       "      <td>1.996884e-04</td>\n",
       "      <td>1.304702e-04</td>\n",
       "      <td>1.202550e-04</td>\n",
       "      <td>7.358295e-05</td>\n",
       "      <td>2.062457e-05</td>\n",
       "      <td>4.204000e-06</td>\n",
       "      <td>4.296666e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>-527.318298</td>\n",
       "      <td>26.068962</td>\n",
       "      <td>-28.148176</td>\n",
       "      <td>10.489081</td>\n",
       "      <td>-19.667913</td>\n",
       "      <td>-5.673967</td>\n",
       "      <td>-8.967707</td>\n",
       "      <td>-12.834253</td>\n",
       "      <td>-10.610332</td>\n",
       "      <td>-4.008108</td>\n",
       "      <td>...</td>\n",
       "      <td>8.703289e-04</td>\n",
       "      <td>8.815533e-04</td>\n",
       "      <td>6.582518e-04</td>\n",
       "      <td>9.203768e-04</td>\n",
       "      <td>5.013914e-04</td>\n",
       "      <td>3.233718e-04</td>\n",
       "      <td>1.378212e-04</td>\n",
       "      <td>2.803395e-05</td>\n",
       "      <td>5.412359e-06</td>\n",
       "      <td>8.532588e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>-314.353607</td>\n",
       "      <td>34.922459</td>\n",
       "      <td>-12.702803</td>\n",
       "      <td>23.294029</td>\n",
       "      <td>-3.972683</td>\n",
       "      <td>3.038771</td>\n",
       "      <td>-4.340724</td>\n",
       "      <td>2.760102</td>\n",
       "      <td>-11.703268</td>\n",
       "      <td>11.975477</td>\n",
       "      <td>...</td>\n",
       "      <td>7.516451e-03</td>\n",
       "      <td>5.683757e-03</td>\n",
       "      <td>8.292719e-03</td>\n",
       "      <td>1.278571e-02</td>\n",
       "      <td>5.974523e-03</td>\n",
       "      <td>1.160816e-03</td>\n",
       "      <td>4.339662e-04</td>\n",
       "      <td>1.370348e-04</td>\n",
       "      <td>2.476061e-05</td>\n",
       "      <td>3.089541e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>-454.633606</td>\n",
       "      <td>38.137886</td>\n",
       "      <td>0.857907</td>\n",
       "      <td>13.460191</td>\n",
       "      <td>-5.483022</td>\n",
       "      <td>6.355078</td>\n",
       "      <td>-3.064339</td>\n",
       "      <td>4.463136</td>\n",
       "      <td>-7.502739</td>\n",
       "      <td>3.806000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.392333e-03</td>\n",
       "      <td>3.421300e-03</td>\n",
       "      <td>2.192394e-03</td>\n",
       "      <td>8.638517e-04</td>\n",
       "      <td>4.565493e-04</td>\n",
       "      <td>2.041313e-04</td>\n",
       "      <td>1.091319e-04</td>\n",
       "      <td>3.348436e-05</td>\n",
       "      <td>6.507279e-06</td>\n",
       "      <td>7.554657e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows Ã— 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0           1          2          3          4          5    \\\n",
       "2466 -395.827271  109.815002  26.334900  34.032913  15.383346   3.612015   \n",
       "2898 -477.751160   95.906578   9.930469   8.222509 -11.132051   2.174797   \n",
       "2807 -635.511292  143.350479  31.454308  36.235615  11.199054  -6.821490   \n",
       "2480 -338.146210  109.372353  13.920223  33.369545   5.132312   2.439946   \n",
       "3433 -444.435394   70.364319  14.762432  19.777376   6.337432  10.598396   \n",
       "...          ...         ...        ...        ...        ...        ...   \n",
       "293  -616.977356   65.106079 -44.349960  19.407906  -2.108910 -20.398224   \n",
       "1773 -485.176361   20.533010 -23.324465  -0.686083 -21.678753  -7.451090   \n",
       "2094 -527.318298   26.068962 -28.148176  10.489081 -19.667913  -5.673967   \n",
       "4561 -314.353607   34.922459 -12.702803  23.294029  -3.972683   3.038771   \n",
       "1408 -454.633606   38.137886   0.857907  13.460191  -5.483022   6.355078   \n",
       "\n",
       "            6          7          8          9    ...           118  \\\n",
       "2466 -21.559561  -1.922961  -1.025764 -10.060621  ...  5.988093e-06   \n",
       "2898 -22.116783   3.972046 -24.781950   1.014994  ...  1.594053e-04   \n",
       "2807  -4.445740   2.376345  -8.011057 -12.235952  ...  1.572271e-08   \n",
       "2480 -28.102674  -6.727801  -0.976052 -15.288651  ...  6.443388e-06   \n",
       "3433 -18.977951   5.523309 -22.413912   1.024355  ...  7.279449e-04   \n",
       "...         ...        ...        ...        ...  ...           ...   \n",
       "293  -19.426634 -10.607098  -6.794960   2.840420  ...  8.187031e-06   \n",
       "1773 -15.483243  -9.952831 -26.001814   2.710965  ...  5.375385e-04   \n",
       "2094  -8.967707 -12.834253 -10.610332  -4.008108  ...  8.703289e-04   \n",
       "4561  -4.340724   2.760102 -11.703268  11.975477  ...  7.516451e-03   \n",
       "1408  -3.064339   4.463136  -7.502739   3.806000  ...  3.392333e-03   \n",
       "\n",
       "               119           120           121           122           123  \\\n",
       "2466  1.842506e-06  5.611226e-07  4.509578e-07  4.783789e-07  5.055595e-07   \n",
       "2898  1.025494e-04  7.045877e-05  5.231352e-05  2.467163e-05  1.023444e-05   \n",
       "2807  9.987369e-09  6.427498e-09  4.529917e-09  3.163819e-09  2.177597e-09   \n",
       "2480  1.800071e-06  9.294707e-07  8.245880e-07  8.740222e-07  9.181483e-07   \n",
       "3433  3.718181e-04  2.150821e-04  1.537552e-04  8.546136e-05  3.408233e-05   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "293   5.235459e-06  2.537881e-06  9.923816e-07  1.467780e-06  1.440692e-06   \n",
       "1773  4.639467e-04  2.838194e-04  1.996884e-04  1.304702e-04  1.202550e-04   \n",
       "2094  8.815533e-04  6.582518e-04  9.203768e-04  5.013914e-04  3.233718e-04   \n",
       "4561  5.683757e-03  8.292719e-03  1.278571e-02  5.974523e-03  1.160816e-03   \n",
       "1408  3.421300e-03  2.192394e-03  8.638517e-04  4.565493e-04  2.041313e-04   \n",
       "\n",
       "               124           125           126           127  \n",
       "2466  5.169114e-07  5.141086e-07  5.031247e-07  4.917418e-07  \n",
       "2898  1.734145e-06  4.149469e-07  8.510043e-08  1.513139e-08  \n",
       "2807  1.774798e-09  1.502753e-09  1.398907e-09  1.364231e-09  \n",
       "2480  9.375869e-07  9.326237e-07  9.153359e-07  8.970123e-07  \n",
       "3433  9.490821e-06  1.508427e-06  2.611371e-07  2.310411e-08  \n",
       "...            ...           ...           ...           ...  \n",
       "293   4.680294e-07  1.425603e-07  2.132506e-08  2.032422e-09  \n",
       "1773  7.358295e-05  2.062457e-05  4.204000e-06  4.296666e-07  \n",
       "2094  1.378212e-04  2.803395e-05  5.412359e-06  8.532588e-07  \n",
       "4561  4.339662e-04  1.370348e-04  2.476061e-05  3.089541e-06  \n",
       "1408  1.091319e-04  3.348436e-05  6.507279e-06  7.554657e-07  \n",
       "\n",
       "[5644 rows x 179 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf=shuffle(newdf)\n",
    "newdf.shape\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf.to_csv(r'E:\\Fusion.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5644, 50), (5644,), (5644, 128))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y =newdf['emotions']\n",
    "x=newdf.iloc[:,0:50]\n",
    "x1=newdf.iloc[:,51:179]\n",
    "x.shape,y.shape,x1.shape\n",
    "\n",
    "\n",
    "##################### To be modified with\n",
    "\n",
    "#newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "#train = rnewdf[newdf1]\n",
    "#test = rnewdf[~newdf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MFCC\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.20,shuffle=False)\n",
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MFCC\n",
    "## CNN\n",
    "\n",
    "#model = Sequential()\n",
    "\n",
    "#model.add(Conv1D(64, 5,padding='same',input_shape=(50,1)))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "#model.add(MaxPooling1D(pool_size=(4)))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "#model.add(MaxPooling1D(pool_size=(4)))\n",
    "#model.add(Conv1D(256, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(7))\n",
    "#model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting emotions to int values MFCC\n",
    "y_traincnn=[]\n",
    "for lab in y_train:\n",
    "    if lab=='neutral':\n",
    "        y_traincnn.append(0)\n",
    "    #elif lab=='calm':\n",
    "        #y_traincnn.append(2)\n",
    "    elif lab=='happy':\n",
    "        y_traincnn.append(1)\n",
    "    elif lab=='sad':\n",
    "        y_traincnn.append(2)\n",
    "    elif lab=='angry':\n",
    "        y_traincnn.append(3)\n",
    "    elif lab=='fear':\n",
    "        y_traincnn.append(4)\n",
    "    elif lab=='disgust':\n",
    "        y_traincnn.append(5)\n",
    "    elif lab=='surprised':\n",
    "        y_traincnn.append(6)\n",
    "y_traincnn=np.asarray(y_traincnn)\n",
    "y_testcnn=[]\n",
    "for lab in y_test:\n",
    "    if lab=='neutral':\n",
    "        y_testcnn.append(0)\n",
    "    #elif lab=='calm':\n",
    "        #y_testcnn.append(2)\n",
    "    elif lab=='happy':\n",
    "        y_testcnn.append(1)\n",
    "    elif lab=='sad':\n",
    "        y_testcnn.append(2)\n",
    "    elif lab=='angry':\n",
    "        y_testcnn.append(3)\n",
    "    elif lab=='fear':\n",
    "        y_testcnn.append(4)\n",
    "    elif lab=='disgust':\n",
    "        y_testcnn.append(5)\n",
    "    elif lab=='surprised':\n",
    "        y_testcnn.append(6)\n",
    "y_testcnn=np.asarray(y_testcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Start CNN MFCC\n",
    "#cnn=model.fit(x_traincnn, y_traincnn, batch_size=16, epochs=500, validation_data=(x_testcnn, y_testcnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(cnn.history['val_loss'])\n",
    "#plt.plot(cnn.history['loss'])\n",
    "#plt.title('loss per epoch')\n",
    "#plt.ylabel('loss')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper right')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(cnn.history['val_accuracy'])\n",
    "#plt.plot(cnn.history['accuracy'])\n",
    "#plt.title('accuracy per epoch')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper right')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(x_testcnn)\n",
    "#predictions\n",
    "#np.argmax(model.predict(x_testcnn),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_testcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report\n",
    "#report = classification_report(y_testcnn, predictions)\n",
    "#print(report)\n",
    "# 0 = neutral,1 = happy, 2 = sad, 3 = angry, 4 = fear, 5 = digust, 6 = surprised,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, acc = model.evaluate(x_testcnn, y_testcnn)\n",
    "#print(\"model, accuracy: {:5.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('E:/testingMFCC_model.h5')\n",
    "#print(\"CNN MFCC MODEL SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "\n",
    "#outputLayer=model.layers[13].output\n",
    "#model.predict(x_traincnn).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEL CNN PART STARTS\n",
    "\n",
    "\n",
    "Xmel_train, Xmel_test, ymel_train, ymel_test = train_test_split(x1,y, test_size = 0.20,shuffle=False)\n",
    "xmel_traincnn = np.expand_dims(Xmel_train, axis=2)\n",
    "xmel_testcnn = np.expand_dims(Xmel_test, axis=2)\n",
    "\n",
    "\n",
    "#xmel_traincnn=np.asarray(xmel_traincnn)\n",
    "#xmel_traincnn\n",
    "##################### To be modified with\n",
    "\n",
    "#newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "#train = rnewdf[newdf1]\n",
    "#test = rnewdf[~newdf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEL CNN model\n",
    "#modelMel = Sequential()\n",
    "\n",
    "#modelMel.add(Conv1D(64, 5,padding='same',input_shape=(128,1)))\n",
    "#modelMel.add(Activation('relu'))\n",
    "#modelMel.add(Dropout(0.1))\n",
    "#modelMel.add(MaxPooling1D(pool_size=(4)))\n",
    "#modelMel.add(Conv1D(128, 5,padding='same',))\n",
    "#modelMel.add(Activation('relu'))\n",
    "#modelMel.add(Dropout(0.1))\n",
    "#modelMel.add(MaxPooling1D(pool_size=(4)))\n",
    "#modelMel.add(Conv1D(256, 5,padding='same',))\n",
    "#modelMel.add(Activation('relu'))\n",
    "#modelMel.add(Dropout(0.1))\n",
    "#modelMel.add(Flatten())\n",
    "#modelMel.add(Dense(7))\n",
    "#modelMel.add(Activation('softmax'))\n",
    "\n",
    "opt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=1e-07, decay=0.0)\n",
    "#def model(inputs):\n",
    "inputs=keras.Input(shape=(50,1),name=\"MFCC\")\n",
    "inputs1=keras.Input(shape=(128,1),name=\"MelSp\")\n",
    "conv1=Conv1D(64,kernel_size=3)(inputs)\n",
    "acti1=Dense(64,activation=\"relu\")(conv1)\n",
    "drop1=Dropout(0.1)(acti1)\n",
    "maxPool1=MaxPooling1D(pool_size=(4))(drop1)\n",
    "conv2=Conv1D(128,kernel_size=3)(maxPool1)\n",
    "acti2=Dense(128,activation=\"relu\")(conv2)\n",
    "drop2=Dropout(0.1)(acti2)\n",
    "maxPool2=MaxPooling1D(pool_size=(4))(drop2)\n",
    "conv3=Conv1D(256,kernel_size=2)(maxPool2)\n",
    "acti3=Dense(256,activation=\"relu\")(conv3)\n",
    "drop3=Dropout(0.1)(acti3)\n",
    "#flat=Flatten()(drop3)\n",
    "\n",
    "flat=Flatten()(drop3)\n",
    "acti4=Dense(7,activation=\"softmax\")(flat)\n",
    "#drop4=Dropout(0.1)(acti4)\n",
    "\n",
    "#flat1=Flatten()(drop4)\n",
    "#dens1=Dense(7)\n",
    "#acti5=Dense(7,activation=\"softmax\")(acti4)\n",
    "\n",
    "\n",
    "#model=Model(inputs=inputs,outputs=acti4)\n",
    "\n",
    "\n",
    "\n",
    "#MEL PART\n",
    "opt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=1e-07, decay=0.0)\n",
    "\n",
    "\n",
    "conv11=Conv1D(64,kernel_size=3)(inputs1)\n",
    "acti11=Dense(64,activation=\"relu\")(conv11)\n",
    "drop11=Dropout(0.1)(acti11)\n",
    "maxPool11=MaxPooling1D(pool_size=(4))(drop11)\n",
    "conv21=Conv1D(128,kernel_size=3)(maxPool11)\n",
    "acti21=Dense(128,activation=\"relu\")(conv21)\n",
    "drop21=Dropout(0.1)(acti21)\n",
    "maxPool21=MaxPooling1D(pool_size=(4))(drop21)\n",
    "conv31=Conv1D(256,kernel_size=2)(maxPool21)\n",
    "acti31=Dense(256,activation=\"relu\")(conv31)\n",
    "drop31=Dropout(0.1)(acti31)\n",
    "\n",
    "flat1=Flatten()(drop31)\n",
    "acti41=Dense(7,activation=\"softmax\")(flat1)\n",
    "#drop4=Dropout(0.1)(acti4)\n",
    "\n",
    "#flat1=Flatten()(drop4)\n",
    "#acti5=Dense(7,activation=\"softmax\")(acti4)\n",
    "\n",
    "fusion=Concatenate()([acti4,acti41])\n",
    "\n",
    "modelFuse=Model(inputs=[inputs,inputs1],outputs=fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFuse.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymel_traincnn=[]\n",
    "for lab in ymel_train:\n",
    "    if lab=='neutral':\n",
    "        ymel_traincnn.append(0)\n",
    "    #elif lab=='calm':\n",
    "        #y_traincnn.append(2)\n",
    "    elif lab=='happy':\n",
    "        ymel_traincnn.append(1)\n",
    "    elif lab=='sad':\n",
    "        ymel_traincnn.append(2)\n",
    "    elif lab=='angry':\n",
    "        ymel_traincnn.append(3)\n",
    "    elif lab=='fear':\n",
    "        ymel_traincnn.append(4)\n",
    "    elif lab=='disgust':\n",
    "        ymel_traincnn.append(5)\n",
    "    elif lab=='surprised':\n",
    "        ymel_traincnn.append(6)\n",
    "ymel_traincnn=np.asarray(ymel_traincnn)\n",
    "ymel_testcnn=[]\n",
    "for lab in ymel_test:\n",
    "    if lab=='neutral':\n",
    "        ymel_testcnn.append(0)\n",
    "    #elif lab=='calm':\n",
    "        #y_testcnn.append(2)\n",
    "    elif lab=='happy':\n",
    "        ymel_testcnn.append(1)\n",
    "    elif lab=='sad':\n",
    "        ymel_testcnn.append(2)\n",
    "    elif lab=='angry':\n",
    "        ymel_testcnn.append(3)\n",
    "    elif lab=='fear':\n",
    "        ymel_testcnn.append(4)\n",
    "    elif lab=='disgust':\n",
    "        ymel_testcnn.append(5)\n",
    "    elif lab=='surprised':\n",
    "        ymel_testcnn.append(6)\n",
    "ymel_testcnn=np.asarray(ymel_testcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "283/283 [==============================] - 5s 11ms/step - loss: 3.5705 - accuracy: 0.2119 - val_loss: 2.1628 - val_accuracy: 0.4588\n",
      "Epoch 2/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 2.3055 - accuracy: 0.4009 - val_loss: 1.9240 - val_accuracy: 0.5518\n",
      "Epoch 3/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 2.0023 - accuracy: 0.5262 - val_loss: 1.8040 - val_accuracy: 0.6138\n",
      "Epoch 4/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.8897 - accuracy: 0.5688 - val_loss: 1.7157 - val_accuracy: 0.6395\n",
      "Epoch 5/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.8138 - accuracy: 0.5803 - val_loss: 1.6565 - val_accuracy: 0.6422\n",
      "Epoch 6/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.7325 - accuracy: 0.6027 - val_loss: 1.6048 - val_accuracy: 0.6678\n",
      "Epoch 7/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.6590 - accuracy: 0.6353 - val_loss: 1.5862 - val_accuracy: 0.6678\n",
      "Epoch 8/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.6340 - accuracy: 0.6462 - val_loss: 1.5479 - val_accuracy: 0.6767\n",
      "Epoch 9/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.5947 - accuracy: 0.6627 - val_loss: 1.5316 - val_accuracy: 0.6776\n",
      "Epoch 10/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.5704 - accuracy: 0.6676 - val_loss: 1.4969 - val_accuracy: 0.6971\n",
      "Epoch 11/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.5510 - accuracy: 0.6721 - val_loss: 1.4802 - val_accuracy: 0.6953\n",
      "Epoch 12/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.5317 - accuracy: 0.6698 - val_loss: 1.4578 - val_accuracy: 0.7006\n",
      "Epoch 13/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.5139 - accuracy: 0.6857 - val_loss: 1.4522 - val_accuracy: 0.7139\n",
      "Epoch 14/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.5015 - accuracy: 0.6712 - val_loss: 1.4414 - val_accuracy: 0.7104\n",
      "Epoch 15/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.4630 - accuracy: 0.7025 - val_loss: 1.4235 - val_accuracy: 0.7236\n",
      "Epoch 16/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.4698 - accuracy: 0.7005 - val_loss: 1.3919 - val_accuracy: 0.7298\n",
      "Epoch 17/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.4424 - accuracy: 0.7034 - val_loss: 1.3779 - val_accuracy: 0.7369\n",
      "Epoch 18/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.4509 - accuracy: 0.7055 - val_loss: 1.3800 - val_accuracy: 0.7484\n",
      "Epoch 19/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.4301 - accuracy: 0.7118 - val_loss: 1.3818 - val_accuracy: 0.7396\n",
      "Epoch 20/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3966 - accuracy: 0.7290 - val_loss: 1.3481 - val_accuracy: 0.7520\n",
      "Epoch 21/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.4173 - accuracy: 0.7109 - val_loss: 1.3551 - val_accuracy: 0.7573\n",
      "Epoch 22/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.4169 - accuracy: 0.7106 - val_loss: 1.3378 - val_accuracy: 0.7573\n",
      "Epoch 23/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3743 - accuracy: 0.7280 - val_loss: 1.3514 - val_accuracy: 0.7484\n",
      "Epoch 24/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.3825 - accuracy: 0.7319 - val_loss: 1.3451 - val_accuracy: 0.7449\n",
      "Epoch 25/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3702 - accuracy: 0.7293 - val_loss: 1.3249 - val_accuracy: 0.7520\n",
      "Epoch 26/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3449 - accuracy: 0.7407 - val_loss: 1.3509 - val_accuracy: 0.7511\n",
      "Epoch 27/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3182 - accuracy: 0.7515 - val_loss: 1.3212 - val_accuracy: 0.7626\n",
      "Epoch 28/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3671 - accuracy: 0.7293 - val_loss: 1.3171 - val_accuracy: 0.7688\n",
      "Epoch 29/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.3246 - accuracy: 0.7374 - val_loss: 1.2908 - val_accuracy: 0.7733\n",
      "Epoch 30/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3330 - accuracy: 0.7418 - val_loss: 1.3059 - val_accuracy: 0.7759\n",
      "Epoch 31/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3334 - accuracy: 0.7450 - val_loss: 1.2726 - val_accuracy: 0.7803\n",
      "Epoch 32/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3085 - accuracy: 0.7565 - val_loss: 1.2872 - val_accuracy: 0.7857\n",
      "Epoch 33/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.3108 - accuracy: 0.7596 - val_loss: 1.2807 - val_accuracy: 0.7848\n",
      "Epoch 34/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2892 - accuracy: 0.7663 - val_loss: 1.2877 - val_accuracy: 0.7662\n",
      "Epoch 35/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2862 - accuracy: 0.7692 - val_loss: 1.2704 - val_accuracy: 0.7865\n",
      "Epoch 36/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.2684 - accuracy: 0.7750 - val_loss: 1.2733 - val_accuracy: 0.7901\n",
      "Epoch 37/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2767 - accuracy: 0.7697 - val_loss: 1.2603 - val_accuracy: 0.7936\n",
      "Epoch 38/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2843 - accuracy: 0.7712 - val_loss: 1.2560 - val_accuracy: 0.7936\n",
      "Epoch 39/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2490 - accuracy: 0.7875 - val_loss: 1.2553 - val_accuracy: 0.7989\n",
      "Epoch 40/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2555 - accuracy: 0.7810 - val_loss: 1.2449 - val_accuracy: 0.7927\n",
      "Epoch 41/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2633 - accuracy: 0.7786 - val_loss: 1.2405 - val_accuracy: 0.7972\n",
      "Epoch 42/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2527 - accuracy: 0.7836 - val_loss: 1.2263 - val_accuracy: 0.7998\n",
      "Epoch 43/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2431 - accuracy: 0.7716 - val_loss: 1.2610 - val_accuracy: 0.7892\n",
      "Epoch 44/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.2538 - accuracy: 0.7782 - val_loss: 1.2263 - val_accuracy: 0.8007\n",
      "Epoch 45/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2433 - accuracy: 0.7718 - val_loss: 1.2445 - val_accuracy: 0.7803\n",
      "Epoch 46/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2324 - accuracy: 0.7775 - val_loss: 1.2380 - val_accuracy: 0.7936\n",
      "Epoch 47/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2180 - accuracy: 0.7894 - val_loss: 1.2176 - val_accuracy: 0.8060\n",
      "Epoch 48/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2194 - accuracy: 0.7990 - val_loss: 1.2717 - val_accuracy: 0.7857\n",
      "Epoch 49/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2273 - accuracy: 0.7843 - val_loss: 1.2141 - val_accuracy: 0.8051\n",
      "Epoch 50/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2203 - accuracy: 0.7909 - val_loss: 1.2082 - val_accuracy: 0.8087\n",
      "Epoch 51/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.2204 - accuracy: 0.7825 - val_loss: 1.2052 - val_accuracy: 0.8113\n",
      "Epoch 52/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.2147 - accuracy: 0.7955 - val_loss: 1.2017 - val_accuracy: 0.8140\n",
      "Epoch 53/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2044 - accuracy: 0.7996 - val_loss: 1.1908 - val_accuracy: 0.8087\n",
      "Epoch 54/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1908 - accuracy: 0.7943 - val_loss: 1.2056 - val_accuracy: 0.8105\n",
      "Epoch 55/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1786 - accuracy: 0.8143 - val_loss: 1.2028 - val_accuracy: 0.8167\n",
      "Epoch 56/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1846 - accuracy: 0.8023 - val_loss: 1.2333 - val_accuracy: 0.7936\n",
      "Epoch 57/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.2032 - accuracy: 0.7946 - val_loss: 1.1953 - val_accuracy: 0.8096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1648 - accuracy: 0.8093 - val_loss: 1.1961 - val_accuracy: 0.8140\n",
      "Epoch 59/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1853 - accuracy: 0.8053 - val_loss: 1.2027 - val_accuracy: 0.8105\n",
      "Epoch 60/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1602 - accuracy: 0.8188 - val_loss: 1.2044 - val_accuracy: 0.8034\n",
      "Epoch 61/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1804 - accuracy: 0.8049 - val_loss: 1.1917 - val_accuracy: 0.8131\n",
      "Epoch 62/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1593 - accuracy: 0.8184 - val_loss: 1.1990 - val_accuracy: 0.8167\n",
      "Epoch 63/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1423 - accuracy: 0.8204 - val_loss: 1.1751 - val_accuracy: 0.8246\n",
      "Epoch 64/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1697 - accuracy: 0.8153 - val_loss: 1.1697 - val_accuracy: 0.8273\n",
      "Epoch 65/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1690 - accuracy: 0.8071 - val_loss: 1.1785 - val_accuracy: 0.8211\n",
      "Epoch 66/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1495 - accuracy: 0.8138 - val_loss: 1.1888 - val_accuracy: 0.8193\n",
      "Epoch 67/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1607 - accuracy: 0.8186 - val_loss: 1.1770 - val_accuracy: 0.8184\n",
      "Epoch 68/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1614 - accuracy: 0.8174 - val_loss: 1.1605 - val_accuracy: 0.8273\n",
      "Epoch 69/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1487 - accuracy: 0.8185 - val_loss: 1.1858 - val_accuracy: 0.8175\n",
      "Epoch 70/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1511 - accuracy: 0.8198 - val_loss: 1.1739 - val_accuracy: 0.8202\n",
      "Epoch 71/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1220 - accuracy: 0.8299 - val_loss: 1.1705 - val_accuracy: 0.8273\n",
      "Epoch 72/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1237 - accuracy: 0.8332 - val_loss: 1.1700 - val_accuracy: 0.8229\n",
      "Epoch 73/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1487 - accuracy: 0.8216 - val_loss: 1.1717 - val_accuracy: 0.8220\n",
      "Epoch 74/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1340 - accuracy: 0.8307 - val_loss: 1.1521 - val_accuracy: 0.8246\n",
      "Epoch 75/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1383 - accuracy: 0.8202 - val_loss: 1.1731 - val_accuracy: 0.8211\n",
      "Epoch 76/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1104 - accuracy: 0.8345 - val_loss: 1.1947 - val_accuracy: 0.8060\n",
      "Epoch 77/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1389 - accuracy: 0.8235 - val_loss: 1.1709 - val_accuracy: 0.8282\n",
      "Epoch 78/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1019 - accuracy: 0.8328 - val_loss: 1.1745 - val_accuracy: 0.8113\n",
      "Epoch 79/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1523 - accuracy: 0.8267 - val_loss: 1.1552 - val_accuracy: 0.8229\n",
      "Epoch 80/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1262 - accuracy: 0.8351 - val_loss: 1.1631 - val_accuracy: 0.8167\n",
      "Epoch 81/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1105 - accuracy: 0.8283 - val_loss: 1.1711 - val_accuracy: 0.8273\n",
      "Epoch 82/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1137 - accuracy: 0.8450 - val_loss: 1.1659 - val_accuracy: 0.8211\n",
      "Epoch 83/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1105 - accuracy: 0.8373 - val_loss: 1.1759 - val_accuracy: 0.8237\n",
      "Epoch 84/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.1062 - accuracy: 0.8384 - val_loss: 1.1498 - val_accuracy: 0.8246\n",
      "Epoch 85/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.1108 - accuracy: 0.8353 - val_loss: 1.1787 - val_accuracy: 0.8140\n",
      "Epoch 86/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0926 - accuracy: 0.8425 - val_loss: 1.1673 - val_accuracy: 0.8131\n",
      "Epoch 87/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0796 - accuracy: 0.8489 - val_loss: 1.1798 - val_accuracy: 0.8167\n",
      "Epoch 88/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.0799 - accuracy: 0.8451 - val_loss: 1.1498 - val_accuracy: 0.8291\n",
      "Epoch 89/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.0762 - accuracy: 0.8462 - val_loss: 1.1650 - val_accuracy: 0.8326\n",
      "Epoch 90/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.0920 - accuracy: 0.8368 - val_loss: 1.1494 - val_accuracy: 0.8317\n",
      "Epoch 91/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0967 - accuracy: 0.8408 - val_loss: 1.1531 - val_accuracy: 0.8237\n",
      "Epoch 92/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0783 - accuracy: 0.8440 - val_loss: 1.1394 - val_accuracy: 0.8291\n",
      "Epoch 93/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0620 - accuracy: 0.8466 - val_loss: 1.1725 - val_accuracy: 0.8255\n",
      "Epoch 94/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0666 - accuracy: 0.8564 - val_loss: 1.1428 - val_accuracy: 0.8273\n",
      "Epoch 95/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0725 - accuracy: 0.8518 - val_loss: 1.1389 - val_accuracy: 0.8308\n",
      "Epoch 96/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0563 - accuracy: 0.8500 - val_loss: 1.1498 - val_accuracy: 0.8317\n",
      "Epoch 97/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0767 - accuracy: 0.8510 - val_loss: 1.1507 - val_accuracy: 0.8273\n",
      "Epoch 98/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0543 - accuracy: 0.8522 - val_loss: 1.1406 - val_accuracy: 0.8317\n",
      "Epoch 99/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0648 - accuracy: 0.8579 - val_loss: 1.1630 - val_accuracy: 0.8229\n",
      "Epoch 100/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0434 - accuracy: 0.8645 - val_loss: 1.1702 - val_accuracy: 0.8237\n",
      "Epoch 101/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0554 - accuracy: 0.8571 - val_loss: 1.1546 - val_accuracy: 0.8229\n",
      "Epoch 102/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0490 - accuracy: 0.8613 - val_loss: 1.1623 - val_accuracy: 0.8229\n",
      "Epoch 103/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0582 - accuracy: 0.8549 - val_loss: 1.1595 - val_accuracy: 0.8282\n",
      "Epoch 104/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0295 - accuracy: 0.8638 - val_loss: 1.1575 - val_accuracy: 0.8255\n",
      "Epoch 105/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0280 - accuracy: 0.8720 - val_loss: 1.1597 - val_accuracy: 0.8237\n",
      "Epoch 106/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.0552 - accuracy: 0.8552 - val_loss: 1.1671 - val_accuracy: 0.8193\n",
      "Epoch 107/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0502 - accuracy: 0.8648 - val_loss: 1.1474 - val_accuracy: 0.8291\n",
      "Epoch 108/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0479 - accuracy: 0.8620 - val_loss: 1.1282 - val_accuracy: 0.8397\n",
      "Epoch 109/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0043 - accuracy: 0.8749 - val_loss: 1.1625 - val_accuracy: 0.8220\n",
      "Epoch 110/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0453 - accuracy: 0.8599 - val_loss: 1.1530 - val_accuracy: 0.8220\n",
      "Epoch 111/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0396 - accuracy: 0.8702 - val_loss: 1.1443 - val_accuracy: 0.8335\n",
      "Epoch 112/200\n",
      "283/283 [==============================] - 2s 9ms/step - loss: 1.0306 - accuracy: 0.8638 - val_loss: 1.1478 - val_accuracy: 0.8237\n",
      "Epoch 113/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0279 - accuracy: 0.8643 - val_loss: 1.1620 - val_accuracy: 0.8220\n",
      "Epoch 114/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0331 - accuracy: 0.8654 - val_loss: 1.1266 - val_accuracy: 0.8344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0195 - accuracy: 0.8763 - val_loss: 1.1717 - val_accuracy: 0.8202\n",
      "Epoch 116/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0141 - accuracy: 0.8788 - val_loss: 1.1368 - val_accuracy: 0.8255\n",
      "Epoch 117/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0481 - accuracy: 0.8641 - val_loss: 1.1580 - val_accuracy: 0.8237\n",
      "Epoch 118/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0279 - accuracy: 0.8687 - val_loss: 1.1555 - val_accuracy: 0.8220\n",
      "Epoch 119/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0358 - accuracy: 0.8678 - val_loss: 1.1494 - val_accuracy: 0.8326\n",
      "Epoch 120/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0408 - accuracy: 0.8617 - val_loss: 1.1501 - val_accuracy: 0.8202\n",
      "Epoch 121/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0251 - accuracy: 0.8728 - val_loss: 1.1612 - val_accuracy: 0.8273\n",
      "Epoch 122/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9922 - accuracy: 0.8793 - val_loss: 1.1531 - val_accuracy: 0.8326\n",
      "Epoch 123/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0264 - accuracy: 0.8679 - val_loss: 1.1391 - val_accuracy: 0.8370\n",
      "Epoch 124/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.0112 - accuracy: 0.8811 - val_loss: 1.1431 - val_accuracy: 0.8423\n",
      "Epoch 125/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9965 - accuracy: 0.8837 - val_loss: 1.1429 - val_accuracy: 0.8344\n",
      "Epoch 126/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9930 - accuracy: 0.8767 - val_loss: 1.1361 - val_accuracy: 0.8406\n",
      "Epoch 127/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9885 - accuracy: 0.8859 - val_loss: 1.1445 - val_accuracy: 0.8246\n",
      "Epoch 128/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9966 - accuracy: 0.8765 - val_loss: 1.1533 - val_accuracy: 0.8211\n",
      "Epoch 129/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0056 - accuracy: 0.8732 - val_loss: 1.1337 - val_accuracy: 0.8415\n",
      "Epoch 130/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 1.0045 - accuracy: 0.8761 - val_loss: 1.1353 - val_accuracy: 0.8361\n",
      "Epoch 131/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9762 - accuracy: 0.8888 - val_loss: 1.1411 - val_accuracy: 0.8370\n",
      "Epoch 132/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9802 - accuracy: 0.8865 - val_loss: 1.1294 - val_accuracy: 0.8397\n",
      "Epoch 133/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 1.0029 - accuracy: 0.8773 - val_loss: 1.1758 - val_accuracy: 0.8291\n",
      "Epoch 134/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9843 - accuracy: 0.8867 - val_loss: 1.1250 - val_accuracy: 0.8441\n",
      "Epoch 135/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9837 - accuracy: 0.8903 - val_loss: 1.1529 - val_accuracy: 0.8299\n",
      "Epoch 136/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9698 - accuracy: 0.8893 - val_loss: 1.1596 - val_accuracy: 0.8335\n",
      "Epoch 137/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9896 - accuracy: 0.8847 - val_loss: 1.1315 - val_accuracy: 0.8388\n",
      "Epoch 138/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9958 - accuracy: 0.8882 - val_loss: 1.1359 - val_accuracy: 0.8264\n",
      "Epoch 139/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9812 - accuracy: 0.8897 - val_loss: 1.1514 - val_accuracy: 0.8291\n",
      "Epoch 140/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9710 - accuracy: 0.8975 - val_loss: 1.1373 - val_accuracy: 0.8415\n",
      "Epoch 141/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9716 - accuracy: 0.8890 - val_loss: 1.1378 - val_accuracy: 0.8459\n",
      "Epoch 142/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9587 - accuracy: 0.8947 - val_loss: 1.1939 - val_accuracy: 0.8158\n",
      "Epoch 143/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9800 - accuracy: 0.8877 - val_loss: 1.1413 - val_accuracy: 0.8450\n",
      "Epoch 144/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9590 - accuracy: 0.8953 - val_loss: 1.1456 - val_accuracy: 0.8353\n",
      "Epoch 145/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9620 - accuracy: 0.8894 - val_loss: 1.1532 - val_accuracy: 0.8379\n",
      "Epoch 146/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9737 - accuracy: 0.8920 - val_loss: 1.1568 - val_accuracy: 0.8308\n",
      "Epoch 147/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9565 - accuracy: 0.8961 - val_loss: 1.1557 - val_accuracy: 0.8317\n",
      "Epoch 148/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9618 - accuracy: 0.8927 - val_loss: 1.1432 - val_accuracy: 0.8379\n",
      "Epoch 149/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9712 - accuracy: 0.8896 - val_loss: 1.1594 - val_accuracy: 0.8353\n",
      "Epoch 150/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9403 - accuracy: 0.9052 - val_loss: 1.1892 - val_accuracy: 0.8264\n",
      "Epoch 151/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9549 - accuracy: 0.8972 - val_loss: 1.1364 - val_accuracy: 0.8344\n",
      "Epoch 152/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9577 - accuracy: 0.8979 - val_loss: 1.1465 - val_accuracy: 0.8406\n",
      "Epoch 153/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9665 - accuracy: 0.8886 - val_loss: 1.1338 - val_accuracy: 0.8415\n",
      "Epoch 154/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9406 - accuracy: 0.9002 - val_loss: 1.1704 - val_accuracy: 0.8353\n",
      "Epoch 155/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9571 - accuracy: 0.8998 - val_loss: 1.1627 - val_accuracy: 0.8326\n",
      "Epoch 156/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9498 - accuracy: 0.9004 - val_loss: 1.1550 - val_accuracy: 0.8379\n",
      "Epoch 157/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9372 - accuracy: 0.9031 - val_loss: 1.1479 - val_accuracy: 0.8512\n",
      "Epoch 158/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9535 - accuracy: 0.9013 - val_loss: 1.1716 - val_accuracy: 0.8326\n",
      "Epoch 159/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9542 - accuracy: 0.8895 - val_loss: 1.1487 - val_accuracy: 0.8397\n",
      "Epoch 160/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9307 - accuracy: 0.8996 - val_loss: 1.1527 - val_accuracy: 0.8335\n",
      "Epoch 161/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9731 - accuracy: 0.8933 - val_loss: 1.1848 - val_accuracy: 0.8370\n",
      "Epoch 162/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9449 - accuracy: 0.9025 - val_loss: 1.1817 - val_accuracy: 0.8282\n",
      "Epoch 163/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9517 - accuracy: 0.8976 - val_loss: 1.1754 - val_accuracy: 0.8406\n",
      "Epoch 164/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9234 - accuracy: 0.9114 - val_loss: 1.1422 - val_accuracy: 0.8415\n",
      "Epoch 165/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9312 - accuracy: 0.9067 - val_loss: 1.1583 - val_accuracy: 0.8379\n",
      "Epoch 166/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9278 - accuracy: 0.9087 - val_loss: 1.1522 - val_accuracy: 0.8459\n",
      "Epoch 167/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9338 - accuracy: 0.9036 - val_loss: 1.2211 - val_accuracy: 0.8158\n",
      "Epoch 168/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9264 - accuracy: 0.9055 - val_loss: 1.1292 - val_accuracy: 0.8468\n",
      "Epoch 169/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9265 - accuracy: 0.9064 - val_loss: 1.1512 - val_accuracy: 0.8406\n",
      "Epoch 170/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9266 - accuracy: 0.9126 - val_loss: 1.1473 - val_accuracy: 0.8406\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9167 - accuracy: 0.9069 - val_loss: 1.1820 - val_accuracy: 0.8308\n",
      "Epoch 172/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9408 - accuracy: 0.9039 - val_loss: 1.1470 - val_accuracy: 0.8353\n",
      "Epoch 173/200\n",
      "283/283 [==============================] - 2s 8ms/step - loss: 0.9177 - accuracy: 0.9104 - val_loss: 1.1463 - val_accuracy: 0.8370\n",
      "Epoch 174/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9473 - accuracy: 0.8952 - val_loss: 1.1576 - val_accuracy: 0.8397\n",
      "Epoch 175/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9091 - accuracy: 0.9162 - val_loss: 1.1562 - val_accuracy: 0.8406\n",
      "Epoch 176/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9248 - accuracy: 0.9092 - val_loss: 1.1740 - val_accuracy: 0.8326\n",
      "Epoch 177/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9463 - accuracy: 0.8978 - val_loss: 1.1879 - val_accuracy: 0.8361\n",
      "Epoch 178/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9350 - accuracy: 0.9068 - val_loss: 1.1585 - val_accuracy: 0.8406\n",
      "Epoch 179/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9116 - accuracy: 0.9101 - val_loss: 1.1597 - val_accuracy: 0.8397\n",
      "Epoch 180/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9366 - accuracy: 0.9061 - val_loss: 1.1715 - val_accuracy: 0.8379\n",
      "Epoch 181/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9182 - accuracy: 0.9129 - val_loss: 1.1732 - val_accuracy: 0.8415\n",
      "Epoch 182/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9120 - accuracy: 0.9137 - val_loss: 1.1655 - val_accuracy: 0.8423\n",
      "Epoch 183/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9043 - accuracy: 0.9186 - val_loss: 1.1435 - val_accuracy: 0.8450\n",
      "Epoch 184/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9059 - accuracy: 0.9178 - val_loss: 1.1564 - val_accuracy: 0.8459\n",
      "Epoch 185/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9022 - accuracy: 0.9165 - val_loss: 1.1581 - val_accuracy: 0.8273\n",
      "Epoch 186/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9095 - accuracy: 0.9256 - val_loss: 1.1394 - val_accuracy: 0.8415\n",
      "Epoch 187/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9124 - accuracy: 0.9141 - val_loss: 1.1430 - val_accuracy: 0.8494\n",
      "Epoch 188/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9059 - accuracy: 0.9219 - val_loss: 1.1742 - val_accuracy: 0.8379\n",
      "Epoch 189/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8790 - accuracy: 0.9281 - val_loss: 1.2097 - val_accuracy: 0.8361\n",
      "Epoch 190/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8967 - accuracy: 0.9215 - val_loss: 1.1788 - val_accuracy: 0.8344\n",
      "Epoch 191/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9035 - accuracy: 0.9245 - val_loss: 1.2013 - val_accuracy: 0.8344\n",
      "Epoch 192/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8850 - accuracy: 0.9227 - val_loss: 1.1590 - val_accuracy: 0.8388\n",
      "Epoch 193/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8995 - accuracy: 0.9198 - val_loss: 1.1626 - val_accuracy: 0.8415\n",
      "Epoch 194/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8902 - accuracy: 0.9169 - val_loss: 1.1853 - val_accuracy: 0.8353\n",
      "Epoch 195/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8943 - accuracy: 0.9237 - val_loss: 1.1919 - val_accuracy: 0.8335\n",
      "Epoch 196/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8837 - accuracy: 0.9289 - val_loss: 1.1640 - val_accuracy: 0.8361\n",
      "Epoch 197/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.9017 - accuracy: 0.9180 - val_loss: 1.2119 - val_accuracy: 0.8308\n",
      "Epoch 198/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8903 - accuracy: 0.9198 - val_loss: 1.1620 - val_accuracy: 0.8397\n",
      "Epoch 199/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8837 - accuracy: 0.9241 - val_loss: 1.1856 - val_accuracy: 0.8317\n",
      "Epoch 200/200\n",
      "283/283 [==============================] - 2s 7ms/step - loss: 0.8554 - accuracy: 0.9352 - val_loss: 1.1556 - val_accuracy: 0.8477\n"
     ]
    }
   ],
   "source": [
    "## Start CNN\n",
    "#cnn1=modelMel.fit(xmel_traincnn, ymel_traincnn, batch_size=16, epochs=500, validation_data=(xmel_testcnn, ymel_testcnn))\n",
    "\n",
    "cnn=modelFuse.fit([np.array(x_traincnn),np.array(xmel_traincnn)],np.array(ymel_traincnn),batch_size=16,epochs=200,validation_data=([x_testcnn,xmel_testcnn], ymel_testcnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x221ca9b1c10>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x221ca9c60a0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'loss vs epoch')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x221ca9b1e20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn.history['loss'])\n",
    "plt.plot(cnn.history['val_loss'])\n",
    "plt.title('loss vs epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x221caaa9160>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x221caaa9400>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'model vs epoch')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x221caa99520>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn.history['accuracy'])\n",
    "plt.plot(cnn.history['val_accuracy'])\n",
    "plt.title('model vs epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.09554296e-10, 1.00000000e+00, 3.81513682e-10, ...,\n",
       "        1.42472997e-01, 8.60176459e-02, 2.07844540e-01],\n",
       "       [1.57840364e-03, 7.81314731e-01, 1.23702723e-03, ...,\n",
       "        2.36961246e-01, 1.18452996e-01, 9.85753685e-02],\n",
       "       [2.32915883e-14, 1.37923357e-14, 9.81324854e-15, ...,\n",
       "        1.63073093e-01, 1.20741390e-01, 1.51594296e-01],\n",
       "       ...,\n",
       "       [6.04794479e-07, 1.51059476e-06, 1.79347280e-05, ...,\n",
       "        1.63346246e-01, 1.25362501e-01, 1.59055918e-01],\n",
       "       [1.14336884e-09, 1.34712774e-09, 6.69600963e-12, ...,\n",
       "        2.17663333e-01, 8.49119574e-02, 1.79310009e-01],\n",
       "       [6.15688332e-05, 2.11506709e-02, 9.95365990e-05, ...,\n",
       "        1.37536764e-01, 1.44367665e-01, 1.48874894e-01]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Melpredictions = modelFuse.predict([x_testcnn,xmel_testcnn])\n",
    "Melpredictions\n",
    "#np.argmax(modelMel.predict(xmel_testcnn),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 6, ..., 3, 4, 6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ymel_testcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "MFCC (InputLayer)               [(None, 50, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MelSp (InputLayer)              [(None, 128, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 48, 64)       256         MFCC[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 126, 64)      256         MelSp[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 48, 64)       4160        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 126, 64)      4160        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 48, 64)       0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 126, 64)      0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 12, 64)       0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 31, 64)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 10, 128)      24704       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 29, 128)      24704       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10, 128)      16512       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 29, 128)      16512       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10, 128)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 29, 128)      0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 2, 128)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 7, 128)       0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1, 256)       65792       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 6, 256)       65792       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 256)       65792       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 6, 256)       65792       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 256)       0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 6, 256)       0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 256)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7)            1799        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 7)            10759       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 14)           0           dense_3[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 366,990\n",
      "Trainable params: 366,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "modelFuse.summary()\n",
    "keras.utils.plot_model(modelFuse, \"Fusion.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#report = classification_report(ymel_testcnn, Melpredictions)\n",
    "#print(report)\n",
    "# 0 = neutral,1 = happy, 2 = sad, 3 = angry, 4 = fear, 5 = digust, 6 = surprised,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 2ms/step - loss: 1.1556 - accuracy: 0.8477\n",
      "model, accuracy: 84.77%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = modelFuse.evaluate([x_testcnn,xmel_testcnn], ymel_testcnn)\n",
    "print(\"model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
